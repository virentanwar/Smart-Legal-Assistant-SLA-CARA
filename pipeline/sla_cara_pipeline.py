# -*- coding: utf-8 -*-
"""SLA_CARA_Pipeline.ipynb

Automatically generated by Colab.


## Installations, GPU Setup and Imports
"""

import os
import docx2txt
import fitz  # PyMuPDF for PDFs
import nltk
from nltk.tokenize import sent_tokenize
from typing import List, Tuple
import json
import torch
import faiss
import numpy as np
from transformers import AutoTokenizer, AutoModel
from sklearn.preprocessing import normalize
from tqdm import tqdm
import requests
from openai import OpenAI
from sklearn.metrics.pairwise import cosine_similarity
from fastapi import FastAPI, UploadFile, File, Form
from fastapi.responses import JSONResponse
import uvicorn
import os
from tempfile import NamedTemporaryFile
import docx2txt
import fitz  # PyMuPDF for PDFs
from typing import List
from pdfminer.high_level import extract_text
import nltk
from nltk.tokenize import sent_tokenize
import nltk
import spacy
import spacy
from typing import List
nlp = spacy.load("en_core_web_sm")

# Use GPU if available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# === STEP 2: Load FAISS index and ID mapping ===
faiss_index = faiss.read_index("faiss_legalbert_trainv1.index")
with open("index_to_idv1.json") as f: index_to_id = json.load(f)
with open("id_to_title_trainv1.json") as f: id_to_title = json.load(f)
with open("id_to_text_trainv1.json") as f: id_to_text_train = json.load(f)
with open("test_keysv1.json") as f: test_keys = json.load(f)
with open("LegalClauses_outputV5.json") as f: full_data = json.load(f)

# Load the actual list of documents from the "documents" key
docs_list = full_data["documents"]

# Now build the mapping from DocID to document
docid_to_doc = {}
for doc in docs_list:
    doc_id = str(doc.get("DocID"))
    if doc_id:
        docid_to_doc[doc_id] = doc

"""## Read Files and Chunk into individual clause Text"""

# === STEP 1: Read DOCX and PDF Files ===
def extract_text_from_docx(docx_path: str) -> str:
    return docx2txt.process(docx_path)

def extract_text_from_pdf(pdf_path: str) -> str:
    if not pdf_path.lower().endswith(".pdf"):
        raise ValueError(f"Unsupported file type: {pdf_path}. Only PDF files are allowed.")
    doc = fitz.open(pdf_path)
    text = ""
    for page in doc:
        text += page.get_text()
    return text

def clean_text(text):
    text = re.sub(r'(\w)-\s*\n\s*(\w)', r'\1\2', text)

    # Remove multiple dashes: e.g., "-----" → ""
    text = re.sub(r'-{2,}', '', text)

    # Remove standalone dashes surrounded by spaces: e.g., " - " → " "
    text = re.sub(r'\s*-\s*', ' ', text)

    # Replace remaining line breaks with space
    text = re.sub(r'\n+', ' ', text)

    # Clean up extra spaces
    text = re.sub(r'\s{2,}', ' ', text).strip()
    
    return text

try:
    nlp = spacy.load("en_core_web_trf")
except:
    nlp = spacy.load("en_core_web_md")

def chunk_into_paragraphs(text: str) -> List[str]:
    """
    Split a legal contract into meaningful clauses or paragraphs.
    Looks for numbered clause patterns or newline breaks intelligently.
    """
    # Preprocess to ensure consistent spacing
    cleaned_text = text.replace('\r\n', '\n').replace('\n\n', '\n')

    # Split by double newlines or clause numbers (e.g., "1.", "2.", "3.")
    import re
    potential_chunks = re.split(r'\n(?=\d{1,2}\.\s)', cleaned_text)
    
    refined_chunks = []
    for chunk in potential_chunks:
        doc = nlp(chunk.strip())
        if len(doc) > 20:  # Avoid trivial fragments
            refined_chunks.append(doc.text.strip())
    
    return refined_chunks

def extract_ner_entities(text):
    """
    Extract named entities from text and organize them nicely.
    """
    doc = nlp(text)
    entities = []
    for ent in doc.ents:
        entities.append(f"{ent.label_}: {ent.text}")
    return entities

# === STEP 3: Main Interface Function ===
def process_legal_input(file_path: str = None, clause_text: str = None) -> List[str]:
    if file_path:
        ext = os.path.splitext(file_path)[-1].lower()
        if ext == ".pdf":
            raw_text = extract_text_from_pdf(file_path)
        elif ext == ".docx":
            raw_text = extract_text_from_docx(file_path)
        else:
            raise ValueError("Unsupported file format. Please upload a .pdf or .docx file.")
        text = clean_text(text)
        paragraphs = chunk_into_paragraphs(raw_text)
        return paragraphs
    elif clause_text:
        return [clause_text.strip()]
    else:
        raise ValueError("Provide either a file or clause text.")

# Utility: Extract text from PDF or DOCX
def extract_text_from_file(file: UploadFile) -> str:
    suffix = file.filename.split(".")[-1]
    with NamedTemporaryFile(delete=False, suffix=f".{suffix}") as temp:
        temp.write(file.file.read())
        temp_path = temp.name

    if suffix.lower() == "pdf":
        text = ""
        doc = fitz.open(temp_path)
        for page in doc:
            text += page.get_text()
    elif suffix.lower() in ["docx", "doc"]:
        text = docx2txt.process(temp_path)
    else:
        raise ValueError("Unsupported file type")

    os.remove(temp_path)
    return text

# Simple chunking using double newlines or sentence split
def chunk_text(text: str) -> List[str]:
    import re
    chunks = re.split(r'\n\s*\n|(?<=[.!?])\s+', text)
    return [chunk.strip() for chunk in chunks if len(chunk.strip()) > 20]

# Step 1: Load and extract text from a PDF file
def extract_text_from_pdf(pdf_path):
    try:
        text = extract_text(pdf_path)
        return text
    except Exception as e:
        print(f"Error extracting PDF: {e}")
        return ""


import json
import re

def clean_risk_output(raw_response):
    # Join and remove backticks and markdown if present
    if isinstance(raw_response, list):
        raw_response = " ".join(raw_response)
    raw_response = raw_response.strip("` \n")

    # Attempt to extract JSON block
    try:
        # Remove all non-JSON content
        match = re.search(r'{.*}', raw_response, re.DOTALL)
        if match:
            clean_json_str = match.group()
            return json.loads(clean_json_str)
        else:
            print("⚠️ JSON not found in response.")
            return None
    except Exception as e:
        print(f"Failed to parse risk response: {e}")
        return None

"""## Embedding Creation with LegalBert"""

embedding_model = AutoModel.from_pretrained("nlpaueb/legal-bert-base-uncased").to(device)
tokenizer = AutoTokenizer.from_pretrained("nlpaueb/legal-bert-base-uncased")

# === EMBEDDING HELPERS ===
def mean_pooling(model_output, attention_mask):
    token_embeddings = model_output[0]
    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)

def get_embedding(text):
    encoded_input = tokenizer(text, return_tensors="pt", padding=True, truncation=True).to(device)
    with torch.no_grad():
        model_output = embedding_model(**encoded_input)
    pooled = mean_pooling(model_output, encoded_input["attention_mask"])
    return normalize(pooled.cpu().numpy())

"""## Retrieve Context from FAISS Vector Database with Cosine Similarity"""

# === STEP 3: Context Retrieval ===
def retrieve_context(query_text, k=5, sim_threshold=0.4):
    inputs = tokenizer(query_text, return_tensors="pt", truncation=True).to(device)
    with torch.no_grad():
        model_output = embedding_model(**inputs)
    query_emb = mean_pooling(model_output, inputs["attention_mask"]).cpu().numpy()
    query_emb = normalize(query_emb)

    D, I = faiss_index.search(query_emb.astype("float32"), k=k * 2)
    retrieved = []

    # Use stored embeddings instead of recomputing (optional optimization)
    for i in I[0]:
        key = index_to_id[i]
        clause_text = id_to_text_train[key]['clause']
        candidate_emb = get_embedding(clause_text)
        cosine_score = float(cosine_similarity(query_emb, candidate_emb)[0][0])

        if cosine_score >= sim_threshold:
            doc = id_to_text_train.get(key, {})
            title = doc.get("title", "Unknown Title")
            summary = doc.get("risk_summary", {}).get("summary", "No risk summary available.")
            retrieved.append({
                "title": title,
                "risk_summary": summary,
                "similarity": cosine_score
            })
        if len(retrieved) >= k:
            break

    return retrieved

def retrieve_context2(query_text, k=5, sim_threshold=0.4):
    inputs = tokenizer(query_text, return_tensors="pt", truncation=True).to(device)
    with torch.no_grad():
        model_output = embedding_model(**inputs)
    query_emb = mean_pooling(model_output, inputs["attention_mask"]).cpu().numpy()
    query_emb = normalize(query_emb)

    D, I = faiss_index.search(query_emb.astype("float32"), k=k * 2)
    retrieved = []

    for i in I[0]:
        key = index_to_id[i]
        clause_text = id_to_text_train[key]['clause']
        candidate_emb = get_embedding(clause_text)
        cosine_score = float(cosine_similarity(query_emb, candidate_emb)[0][0])

        if cosine_score >= sim_threshold:
            doc_id = key.split("_")[0]
            doc = docid_to_doc.get(doc_id)

            title = "Unknown Title"
            summary_text = "No summary available."

            if doc:
                title = doc.get("title", "Unknown Title")
                for para in doc.get("paragraphs", []):
                    for qa in para.get("qas", []):
                        if "answers" in qa and isinstance(qa["answers"], list) and len(qa["answers"]) > 0:
                            candidate_summary = qa["answers"][0].get("text", "").strip()
                            if len(candidate_summary.split()) > 8:
                                summary_text = candidate_summary
                                break
                    if summary_text != "No summary available.":
                        break

            retrieved.append({
                "title": title,
                "summary": summary_text,
                "similarity": cosine_score
            })

        if len(retrieved) >= k:
            break

    return retrieved

def retrieve_context3(query_text, k=5, sim_threshold=0.4):
    inputs = tokenizer(query_text, return_tensors="pt", truncation=True).to(device)
    with torch.no_grad():
        model_output = embedding_model(**inputs)
    query_emb = mean_pooling(model_output, inputs["attention_mask"]).cpu().numpy()
    query_emb = normalize(query_emb)

    D, I = faiss_index.search(query_emb.astype("float32"), k=k * 2)
    retrieved = []

    for i in I[0]:
        key = index_to_id[i]
        clause_text = id_to_text_train[key]['clause']
        candidate_emb = get_embedding(clause_text)
        cosine_score = float(cosine_similarity(query_emb, candidate_emb)[0][0])

        if cosine_score >= sim_threshold:
            doc_id = key.split("_")[0]
            doc = docid_to_doc.get(doc_id)

            title = "Unknown Title"
            summary_text = "No summary available."

            if doc:
                title = doc.get("title", "Unknown Title")
                for para in doc.get("paragraphs", []):
                  qa = para.get("qas", [])
                  cqa = para.get("CVSqas", [])
                  retrieved.append({
                "title": title,
                "summary": summary_text,
                "similarity": cosine_score
            })

        if len(retrieved) >= k:
            break

    return retrieved

"""## Setup Llama3.2 3B API Client"""

# === STEP 4: Set up the client for Together API ===
TOGETHER_API_KEY = "secret_cara_53d795114259496d8912629a8e8acb29.aGlZk2nvK4z73NlnQECX0Ixoesx7gPQH"
MODEL_NAME = "llama3.2-3b-instruct"
client = OpenAI(
    api_key=TOGETHER_API_KEY,
    base_url="https://api.lambda.ai/v1"
)

# === STEP 5: LLM CALL ===
def call_together_ai(prompt, max_tokens=250):
    response = client.chat.completions.create(
        model=MODEL_NAME,
        messages=[
            {"role": "system", "content": "You are a legal AI expert. Follow the instructions precisely."},
            {"role": "user", "content": prompt}
        ],
        max_tokens=max_tokens,
        temperature=0.3,
        top_p=0.9,
        stop=["\n\n"]
    )
    return response.choices[0].message.content.strip()

"""## Tasks Setup"""

# === STEP 6: TASK 1 - CLAUSE CLASSIFICATION ===
def classify_clause(clause):
    titles = retrieve_context(clause, sim_threshold=0.6)
    prompt = f"""
You are a legal AI tasked with classifying a contract clause. Use only the clause and the reference titles.

Clause:
{clause}

Reference Titles:
"""
    for ref in titles:
        prompt += f"- {ref['title']} (Similarity: {ref['similarity']:.2f})\n"

    prompt += """
Select the category:
- Affiliate Agreement
- Agency Agreement
- Collaboration/Cooperation Agreement
- Co-Branding Agreement
- Consulting Agreement
- Development Agreement
- Distributor Agreement
- Endorsement Agreement
- Franchise Agreement
- Hosting Agreement
- IP Agreement
- Joint Venture Agreement
- License Agreement
- Maintenance Agreement
- Manufacturing Agreement
- Marketing Agreement
- Non-Compete / No-Solicit / Non-Disparagement Agreement
- Outsourcing Agreement
- Promotion Agreement
- Reseller Agreement
- Service Agreement
- Sponsorship Agreement
- Supply Agreement
- Strategic Alliance Agreement
- Transportation Agreement

Output Format (strictly follow this format): 
    {{"Category": <exact category>,
      "Reasoning": <one-line reason>}}
"""
    return call_together_ai(prompt)

def summarize_term_clause(clause):
  retrieved_context = retrieve_context2(clause)
  prompt = f"""
You are a legal AI assistant. Your task is to summarize the following contract clause in 1–2 clear and accurate sentences.
Your summary should **mirror the wording and details** from the clause wherever possible, but remain **clear and self-contained**. Do **not** reference the clause or say "this clause." Use the **reference context** only to disambiguate or clarify meaning — do not include new content from it.
Instructions:
- Focus **only on the clause content itself**, not the metadata or references.
- Use the reference context *only* to understand unclear terms, not to copy from it.
- If the clause describes one of the following, tailor the summary accordingly:
    • Term: Highlight the duration and start/end of the agreement.
    • Termination: Describe how the contract can be ended by either party.
    • Confidentiality: Summarize obligations to protect information.
    • Governing Law: Mention which law/jurisdiction applies.
    • Payment Terms: Explain payment frequency, due dates, penalties.
    • IP License Grant: Clarify type of license and its scope.

Clause:
\"\"\"{clause}\"\"\"

Reference Context:
{retrieved_context}
Output Format (strictly follow this format): 
    {{"summary": <Summary as above mentioned>}}
Summary:
""".strip()
  return call_together_ai(prompt, max_tokens=200)

def assess_clause_risks(clause):
    retrieved_context = retrieve_context(clause)
    context_str = "\n".join([f"{doc['title']}: {doc.get('summary', '')}" for doc in retrieved_context])

    prompt = f"""
You are a legal AI assistant. Carefully analyze the **contract clause** below to identify legal risks and assign a risk score.

Your task:
1. Evaluate the legal risks mentioned in the clause.
2. Choose only one from the following risk categories:
   - Termination Risk
   - Confidentiality Risk
   - Payment Risk
   - Liability Risk
   - IP Risk
   - Change of Control Risk
   - No Risk

3. Assign a **Risk Score** between 1 (low) and 4 (high), based on how critical the risks are.
4. Return your answer strictly in **valid JSON format** as shown below.

Clause:
\"\"\"{clause}\"\"\"

Context (from similar agreements):
{context_str}

Output Format (strictly follow this format):
{{
  "Risk Score": <score between 1 and 3>,
  "Legal Risks": ["<Only One of the above categories>", "..."]
}}
"""

    response = call_together_ai(prompt, max_tokens=150)
    response = clean_risk_output(response)
    return response

def extract_legal_insights(clause, entities):
    retrieved_context = retrieve_context(clause)
    entities_text = "\n".join(entities) if entities else "No significant named entities found."
    prompt = f"""
You are a legal AI assistant. Analyze the clause below and extract all explicit legal details.

**Your task is to extract ONLY the information clearly stated in the clause**. Use the reference Context just for context, not content.

Extract the following:
- **Key Legal Entities**: Companies, organizations, or individuals explicitly mentioned.
- **Important Dates**: Effective date, expiration, deadlines.
- **Obligations**: Who is required to do what.
- **Parties Involved**: Formal parties to the agreement.

Clause:
\"\"\"{clause}\"\"\"

Recognized Entities:
{entities_text}

Reference Context (for background only, not content):
{retrieved_context}

Return your answer in the format below. DO NOT hallucinate or summarize. Only include what is directly visible in the clause. Do not include the content from Reference Context in the output.
{{
  "Legal Entities": {{
  "Important Dates": <A list of dates extracted as above>,
  "Legal Entities": <A list of Legal Entities extracted as above>,
  "Obligations": <A list of Obligations extracted as above>,
  "Parties Involved": <A list of Parties Involved extracted as above>
  }}}}
"""
    return call_together_ai(prompt)

def extract_legal_insightssingle(clause):
    retrieved_context = retrieve_context(clause)
    prompt = f"""
You are a legal AI assistant. Analyze the clause below and extract all explicit legal details.

**Your task is to extract ONLY the information clearly stated in the clause**. Use the reference Context just for context, not content.

Extract the following:
- **Key Legal Entities**: Companies, organizations, or individuals explicitly mentioned.
- **Important Dates**: Effective date, expiration, deadlines.
- **Obligations**: Who is required to do what.
- **Parties Involved**: Formal parties to the agreement.

Clause:
\"\"\"{clause}\"\"\"

Reference Context (for background only, not content):
{retrieved_context}

Return your answer in the format below. DO NOT hallucinate or summarize. Only include what is directly visible in the clause. Do not include the content from Reference Context in the output.
{{
  "Legal Entities": {{
  "Important Dates": <A list of dates extracted as above>,
  "Legal Entities": <A list of Legal Entities extracted as above>,
  "Obligations": <A list of Obligations extracted as above>,
  "Parties Involved": <A list of Parties Involved extracted as above>
  }}}}
"""
    return call_together_ai(prompt)

"""## Pipeline Setup"""

def run_pipeline(input_data, is_file=True):
    results = []
    if is_file:
        paragraphs = extract_text_from_pdf(input_data)
        if paragraphs:
            entities = extract_ner_entities(paragraphs)
            clause_paragraphs = chunk_into_paragraphs(paragraphs)
            for i, clause in enumerate(clause_paragraphs):
                result = {
                    "clauseid": i,
                    "clause": clause,
                    "classification": classify_clause(clause),
                    "risk_analysis": assess_clause_risks(clause),
                    "summary": summarize_term_clause(clause),
                    "legal_extraction": extract_legal_insights(clause, entities)
                }
                results.append(result)
    else:
        single_clause = input_data
        result = {
            "clause": single_clause,
            "classification": classify_clause(single_clause),
            "risk_analysis": assess_clause_risks(single_clause),
            "summary": summarize_term_clause(single_clause),
            "legal_extraction": extract_legal_insightssingle(single_clause)
        }
        return result
    return results



